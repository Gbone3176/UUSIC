import torch
import torch.nn as nn
import time
import threading
import numpy as np
from datetime import datetime
import os

os.environ["CUDA_VISIBLE_DEVICES"] = "7"

class GPUOccupier:
    def __init__(self, target_memory_gb=20, target_utilization=0.6):
        """
        GPUå ç”¨ç¨‹åº
        
        Args:
            target_memory_gb: ç›®æ ‡æ˜¾å­˜å ç”¨ï¼ˆGBï¼‰
            target_utilization: ç›®æ ‡åˆ©ç”¨ç‡ï¼ˆ0-1ä¹‹é—´ï¼‰
        """
        self.target_memory_gb = target_memory_gb
        self.target_utilization = target_utilization
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.running = False
        self.memory_tensors = []
        
        if not torch.cuda.is_available():
            print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•å ç”¨GPU")
            return
            
        print(f"ğŸ¯ ç›®æ ‡: å ç”¨ {target_memory_gb}GB æ˜¾å­˜, åˆ©ç”¨ç‡ {target_utilization*100}%")
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {torch.cuda.get_device_name(0)}")
        
    def occupy_memory(self):
        """å ç”¨æŒ‡å®šå¤§å°çš„æ˜¾å­˜"""
        print("ğŸ’¾ å¼€å§‹å ç”¨æ˜¾å­˜...")
        
        # è®¡ç®—éœ€è¦åˆ†é…çš„æ˜¾å­˜å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        target_bytes = self.target_memory_gb * 1024**3
        
        # åˆ†å—åˆ†é…æ˜¾å­˜ï¼Œé¿å…ä¸€æ¬¡æ€§åˆ†é…è¿‡å¤§
        chunk_size = 1024**3  # 1GB per chunk
        num_chunks = int(target_bytes // chunk_size)
        remaining_bytes = target_bytes % chunk_size
        
        try:
            # åˆ†é…æ•´æ•°GBçš„å—
            for i in range(num_chunks):
                chunk = torch.randn(chunk_size // 4, device=self.device, dtype=torch.float32)
                self.memory_tensors.append(chunk)
                current_memory = torch.cuda.memory_allocated() / 1024**3
                print(f"  å·²åˆ†é…: {current_memory:.2f}GB")
                
            # åˆ†é…å‰©ä½™çš„æ˜¾å­˜
            if remaining_bytes > 0:
                chunk = torch.randn(remaining_bytes // 4, device=self.device, dtype=torch.float32)
                self.memory_tensors.append(chunk)
                
            final_memory = torch.cuda.memory_allocated() / 1024**3
            print(f"âœ… æ˜¾å­˜å ç”¨å®Œæˆ: {final_memory:.2f}GB")
            
        except RuntimeError as e:
            current_memory = torch.cuda.memory_allocated() / 1024**3
            print(f"âš ï¸  æ˜¾å­˜åˆ†é…å—é™: {current_memory:.2f}GB (é”™è¯¯: {e})")
    
    def create_dummy_model(self):
        """åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ¨¡å‹ç”¨äºäº§ç”Ÿè®¡ç®—è´Ÿè½½"""
        class DummyModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.layers = nn.Sequential(
                    nn.Linear(1024, 2048),
                    nn.ReLU(),
                    nn.Linear(2048, 4096),
                    nn.ReLU(),
                    nn.Linear(4096, 2048),
                    nn.ReLU(),
                    nn.Linear(2048, 1024),
                    nn.ReLU(),
                    nn.Linear(1024, 512),
                )
                
            def forward(self, x):
                return self.layers(x)
        
        return DummyModel().to(self.device)
    
    def gpu_computation_worker(self):
        """GPUè®¡ç®—å·¥ä½œçº¿ç¨‹"""
        model = self.create_dummy_model()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        
        # è®¡ç®—å·¥ä½œå’Œä¼‘æ¯æ—¶é—´
        work_time = self.target_utilization  # å·¥ä½œæ—¶é—´æ¯”ä¾‹
        rest_time = 1 - self.target_utilization  # ä¼‘æ¯æ—¶é—´æ¯”ä¾‹
        cycle_duration = 1.0  # æ¯ä¸ªå‘¨æœŸ1ç§’
        
        work_duration = cycle_duration * work_time
        rest_duration = cycle_duration * rest_time
        
        print(f"ğŸ”„ å¼€å§‹è®¡ç®—è´Ÿè½½: å·¥ä½œ{work_duration:.2f}s, ä¼‘æ¯{rest_duration:.2f}s")
        
        iteration = 0
        while self.running:
            cycle_start = time.time()
            
            # å·¥ä½œé˜¶æ®µ
            work_start = time.time()
            while time.time() - work_start < work_duration and self.running:
                # ç”Ÿæˆéšæœºè¾“å…¥
                batch_size = 256
                input_data = torch.randn(batch_size, 1024, device=self.device)
                target = torch.randn(batch_size, 512, device=self.device)
                
                # å‰å‘ä¼ æ’­
                output = model(input_data)
                loss = nn.MSELoss()(output, target)
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                iteration += 1
                if iteration % 10000 == 0:
                    current_time = datetime.now().strftime("%H:%M:%S")
                    memory_used = torch.cuda.memory_allocated() / 1024**3
                    # print(f"[{current_time}] è¿­ä»£: {iteration}, æ˜¾å­˜: {memory_used:.2f}GB, æŸå¤±: {loss.item():.4f}")
            
            # ä¼‘æ¯é˜¶æ®µ
            if rest_duration > 0 and self.running:
                time.sleep(rest_duration)
    
    def start(self):
        """å¼€å§‹å ç”¨GPU"""
        if not torch.cuda.is_available():
            return
            
        print("ğŸš€ å¯åŠ¨GPUå ç”¨ç¨‹åº...")
        print("-" * 60)
        
        # å…ˆå ç”¨æ˜¾å­˜
        self.occupy_memory()
        
        # å¯åŠ¨è®¡ç®—çº¿ç¨‹
        self.running = True
        self.compute_thread = threading.Thread(target=self.gpu_computation_worker)
        self.compute_thread.daemon = True
        self.compute_thread.start()
        
        print("âœ… GPUå ç”¨ç¨‹åºå·²å¯åŠ¨")
        print("ğŸ’¡ æŒ‰ Ctrl+C é€€å‡ºç¨‹åº")
        print("-" * 60)
        
        try:
            # ä¸»çº¿ç¨‹ä¿æŒè¿è¡Œï¼Œå®šæœŸæ˜¾ç¤ºçŠ¶æ€
            while True:
                time.sleep(10)
                memory_used = torch.cuda.memory_allocated() / 1024**3
                memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                memory_percent = (memory_used / memory_total) * 100
                
                current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                print(f"[{current_time}] æ˜¾å­˜ä½¿ç”¨: {memory_used:.2f}GB / {memory_total:.2f}GB ({memory_percent:.1f}%)")
                
        except KeyboardInterrupt:
            self.stop()
    
    def stop(self):
        """åœæ­¢å ç”¨GPU"""
        print("\nğŸ›‘ åœæ­¢GPUå ç”¨...")
        self.running = False
        
        # é‡Šæ”¾æ˜¾å­˜
        self.memory_tensors.clear()
        torch.cuda.empty_cache()
        
        final_memory = torch.cuda.memory_allocated() / 1024**3
        print(f"âœ… å·²é‡Šæ”¾æ˜¾å­˜ï¼Œå½“å‰å ç”¨: {final_memory:.2f}GB")
        print("ğŸ‘‹ ç¨‹åºå·²é€€å‡º")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ® GPUå å¡ç¨‹åº v1.0")
    print("=" * 60)
    
    # æ˜¾ç¤ºGPUä¿¡æ¯
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        print(f"ğŸ” æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU:")
        for i in range(gpu_count):
            name = torch.cuda.get_device_name(i)
            total_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"  GPU {i}: {name} ({total_memory:.1f}GB)")
    else:
        print("âŒ æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡")
        return
    
    print("-" * 60)
    
    # åˆ›å»ºå¹¶å¯åŠ¨å å¡ç¨‹åº
    occupier = GPUOccupier(target_memory_gb=25, target_utilization=0.6)
    occupier.start()

if __name__ == "__main__":
    main()
